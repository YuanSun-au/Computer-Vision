{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37164bitgp2conda303b1e298da745dbbce97ed0cfa82691",
   "display_name": "Python 3.7.1 64-bit ('GP2': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision Recap - Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lecture 20: Bayes Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "$$ P(w_j|x) = \\frac{ P(x|w_j) \\; P(w_j) }{ P(x) } $$ \n",
    "\n",
    "where the evidence $P(x)$ is a mixture of Gaussians (this is not a normal distribution anymore):\n",
    "$$ P(x) = \\sum_{k=1} P(x | w_k) P(w_k) $$\n",
    "\n",
    "\n",
    "PDF:\n",
    "$$f\\left(x_{1}, \\ldots, x_{m}\\right)=\\frac{1}{(2 \\pi)^{m / 2} \\sqrt{\\operatorname{det} \\Sigma}} e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\top} \\mathbf{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})}$$\n",
    "\n",
    "Mahalanobis Distance:\n",
    "$$ r = \\sqrt{ (x-\\mu)^\\top \\Sigma^{-1}(x-\\mu) } $$\n",
    "\n",
    "Curse of dimensionality:\n",
    "\n",
    "- if features are not independent, $\\color{red}{exponentially}$ more training data is needed to compute meaninful likelihoods\n",
    "- If features are independent: $P(x|w_j)=P(x_1,\\dots,x_d|w_j)=P(x_1|w_j)\\dots P(x_d|w_j)$, then the effort grows linearly with the dimension.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Bayesian Risk\n",
    "\n",
    "The cost (risk) of a decision is defined by a cost (loss) function $\\lambda(a,w_j|x)$ given by:\n",
    "\n",
    "$$ R[a|x] = \\mathbb E_{w_j \\sim p(w_j|x)}[\\lambda(a,w_j|x)] = \\sum_{j=1} \\lambda(a,w_j|x) \\, p(w_j|x) $$\n",
    "\n",
    "Examples of conditional risks:\n",
    "$$ R\\left(\\alpha_{1} | \\boldsymbol{x}\\right)=\\lambda_{11} P\\left(\\omega_{1} | \\boldsymbol{x}\\right)+\\lambda_{12} P\\left(\\omega_{2} | \\boldsymbol{x}\\right)$$\n",
    "$$ R\\left(\\alpha_{2} | \\boldsymbol{x}\\right)=\\lambda_{21} P\\left(\\omega_{1} | \\boldsymbol{x}\\right)+\\lambda_{22} P\\left(\\omega_{2} | \\boldsymbol{x}\\right)$$\n",
    "\n",
    "$\\lambda_{11}$ and $\\lambda_{22}$ are the cost for correct classification, that might not be zero, but at least we expect $\\lambda_{11} < \\lambda_{21}$ and $\\lambda_{22} < \\lambda_{12}$\n",
    "\n",
    "Bayes decision rule: decide on $w_j$ if $R(a_j|x) < R(a_k|x), \\;\\; \\forall k$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Likelihood Ratio Test (LRT)\n",
    "\n",
    "$$ \\Lambda(\\boldsymbol{x})=\\frac{P\\left(\\boldsymbol{x} | \\omega_{1}\\right)}{P\\left(\\boldsymbol{x} | \\omega_{2}\\right)} \\begin{array}{l} \\omega_{1} \\\\ >\\\\<\\\\ \\omega_{2} \\end{array} \\underbrace{\\frac{\\lambda_{12}-\\lambda_{22}}{\\lambda_{21}-\\lambda_{11}} \\cdot \\frac{P\\left(\\omega_{2}\\right)}{P\\left(\\omega_{1}\\right)}}_{T} $$\n",
    "\n",
    "if $\\lambda_{11}=\\lambda_{22}=1$ and $\\lambda_{12}=\\lambda_{21}=0$, then the LRT is called MAP criterion and ML for equal priors (T=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discriminant Based Classification\n",
    "\n",
    "Discriminant function: $g(x)$. Choosen class $i$ if \n",
    "$$ g_i(x) > g_j(x) \\quad \\forall i \\neq j $$\n",
    "\n",
    "Bayes rule can be used as a DF: $g_i(x) = P(w_i|x)$\n",
    "\n",
    "which implies that:\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\Sigma_{i}^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)-\\frac{d}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln \\left(\\left|\\Sigma_{i}\\right|\\right)+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 1: $\\Sigma_i=\\sigma I$\n",
    "\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{\\left\\|\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right\\|^{2}}{2 \\sigma}=-\\frac{x^{\\top} x-2 \\mu_i^{\\top} x+\\mu_i^{\\top} \\mu_i}{2 \\sigma}$$\n",
    "\n",
    "results in $\\color{red}{linear}$ discriminant (Minimum Euclidean Distance Classifier):\n",
    "$$g_{i}(\\boldsymbol{x})=\\boldsymbol{w}_{i}^{\\top} \\boldsymbol{x}+\\boldsymbol{w}_{i 0} \\quad \\text{where}$$\n",
    "$$\\boldsymbol{w}_{i}=\\frac{1}{\\sigma} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2 \\sigma} \\boldsymbol{\\mu}_{i}^{\\top} \\boldsymbol{\\mu}_{i}+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 2: $\\Sigma_i=\\Sigma$\n",
    "\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\Sigma_{i}^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)$$\n",
    "\n",
    "which is also a $\\color{red}{linear}$ discriminant (Minimum Mahalanobis Distance Classifier):\n",
    "$$g_{i}(\\boldsymbol{x})=\\boldsymbol{w}_{i}^{\\top} \\boldsymbol{x}+\\boldsymbol{w}_{i 0} \\quad \\text{where}$$\n",
    "$$\\boldsymbol{w}_{i}=\\Sigma^{-1} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2} \\boldsymbol{\\mu}_{i}^{\\top} \\Sigma^{-1} \\boldsymbol{\\mu}_{i}+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 3: $\\Sigma_i$ = Arbitrary\n",
    "\n",
    "$$ g_i(x) = x^\\top W_i x + w_i^\\top x + w_{i,0} \\quad \\text{where}$$\n",
    "$$W_{i}=-\\frac{1}{2} \\Sigma_{i}^{-1}, \\quad \\boldsymbol{w}_{i}=\\Sigma_{i}^{-1} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2} \\boldsymbol{\\mu}_{i}^{\\top} \\Sigma_{i}^{-1} \\boldsymbol{\\mu}_{i}-\\frac{1}{2} \\ln \\left|\\Sigma_{i}\\right|+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "which is $\\color{red}{quadratic}$ and non-contiguous\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<br><br>\n",
    "\n",
    "## Lecture 21: Parametric Techniques, Density Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter Estimation\n",
    "\n",
    "#### Maximum Likelihood\n",
    "\n",
    "If samples are drawn independently\n",
    "$$ l(\\boldsymbol{\\theta})=\\ln p(D | \\boldsymbol{\\theta})=\\ln \\prod_{k=1}^{n} p\\left(\\boldsymbol{x}_{k} | \\boldsymbol{\\theta}\\right)=\\sum_{k=1}^{n} \\ln p\\left(\\boldsymbol{x}_{k} | \\boldsymbol{\\theta}\\right) $$\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}}=\\arg \\max _{\\boldsymbol{\\theta}} l(\\boldsymbol{\\theta})$$\n",
    "\n",
    "Maximizing the likelihood implies that:\n",
    "$$\\nabla_{\\theta} l(\\theta)=\\nabla_{\\theta} \\sum_{k=1}^{n} \\ln p\\left(x_{k} | \\theta\\right)=\\sum_{k=1}^{n}\\nabla_{\\theta} \\ln p\\left(x_{k} | \\theta\\right)=0$$\n",
    "\n",
    "If Gaussian case and unknown mean $\\mu$ and unknown covariance $\\Sigma$, then:\n",
    "\n",
    "$$\\hat{\\mu}=\\frac{1}{n} \\sum_{k=1}^{m} x_{k} \\quad \\text{and} \\quad \\hat{\\Sigma}=\\frac{1}{n} \\sum_{k=1}^{n}\\left(x_{k}-\\mu\\right)\\left(x_{k}-\\mu\\right)^{\\top}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Density Estimation\n",
    "\n",
    "Idea: Estimate the distribution (function) from scratch\n",
    "\n",
    "$$P=\\int_{R} p(x) d x \\approx p\\left(x^{*}\\right) \\operatorname{vol}(R) \\approx \\frac{k}{n}$$\n",
    "\n",
    "where $k$ out of $n$ samples fall into the range $R$\n",
    "\n",
    "Choose a large enough R, so that it contains sufficiently many samples and (so that variance/uncertainty is not so large)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/densest.png\" width=400>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "To estimate distribution at $x^*$:\n",
    "$$p_{n}\\left(x^{*}\\right)=\\frac{k_{n} / n}{\\operatorname{vol}\\left(R_{n}\\right)} \\approx p\\left(x^{*}\\right)$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Bias-Variance Tradeoff\n",
    "\n",
    "<img src=\"./images/biasvarianceto.png\" width=600>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "$\\operatorname{MSE}(\\tilde{p}(\\boldsymbol{x})):=E\\left[(\\tilde{p}(\\boldsymbol{x})-p(\\boldsymbol{x}))^{2}\\right]=\\underbrace{(E[\\tilde{p}(\\boldsymbol{x})]-p(\\boldsymbol{x}))^{2}}_{\\text {Bias }}+\\underbrace{E\\left[(\\tilde{p}(\\boldsymbol{x})-E[\\tilde{p}(\\boldsymbol{x})])^{2}\\right]}_{\\text {Variance }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<br><br>\n",
    "\n",
    "## Lecture 22: Non-Parametric Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parzen Windows\n",
    "\n",
    "**Idea:** Count the number of samples $k$ within a region $R$ of fixed size\n",
    "\n",
    "Define a funcion:\n",
    "$$\\varphi(u)=\\left\\{ \\begin{array}{ll} 1, & \\left|u_{j}\\right| \\leq \\frac{1}{2}, j=1,2, \\ldots, d \\\\ 0, & \\text { otherwise }\\end{array} \\right.$$\n",
    "\n",
    "The previous definitions for the density and the number of samples\n",
    "$$\\begin{align*} p(\\boldsymbol{x}) \\approx \\frac{k_{n} / n}{\\operatorname{vol}\\left(R_{n}\\right)} \\quad \\text { and } \\quad k_{n}=\\sum_{i=1}^{n} \\varphi\\left(\\frac{\\boldsymbol{x}-\\boldsymbol{x}_{i}}{h}\\right) \\end{align*}$$\n",
    "\n",
    "in a region $R$ lead to the density estimate\n",
    "$$ \\begin{align*} \\tilde{p}(\\boldsymbol{x}) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{\\operatorname{vol}(R)} \\varphi\\left(\\frac{x-x_{i}}{h}\\right)=\\frac{1}{n h^{d}} \\sum_{i=1}^{n} \\varphi\\left(\\frac{x-x_{i}}{h}\\right) \\end{align*} $$\n",
    "\n",
    "#### General Parzen Windows\n",
    "\n",
    "Change kernel function $\\varphi((x-x_i)/h)$ by a Gaussian centered at $x_i$ with variance $h$\n",
    "\n",
    "<img src=\"./images/parzen.png\" width=400>\n",
    "\n",
    "#### Classification Using Parzen Windows\n",
    "\n",
    "- Estimate likelihoods $p(x|w_i)$ using parzen window approach\n",
    "- Use MAP to classify\n",
    "\n",
    "Problems:\n",
    "- Requires lots of data to ensure convergence of the distribution\n",
    "- Training error can be reduced to zero if window is small enough. However, this will cause overfitting\n",
    "- Curse of dimensionality\n",
    "\n",
    "Solution (Naive Bayes Classifier)\n",
    "\n",
    "Assume class-conditional independence of the features\n",
    "$$p\\left(\\boldsymbol{x} | \\omega_{i}\\right)=\\prod_{j=1}^{d} p\\left(x^{(j)} | \\omega_{i}\\right)$$\n",
    "\n",
    "and the correspondent discriminant function becomes:\n",
    "$$g_{i}^{N B}=p\\left(\\omega_{i}\\right) \\prod_{j=1}^{d} p\\left(x^{(j)} | \\omega_{i}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-Nearest Neighbours (KNN)\n",
    "\n",
    "large window ←→ few data points, small window ←→ dense data points\n",
    "\n",
    "Enlarge the window until a pre-selected number k of samples are enclosed by the window −→ k-nearest neighbours of x\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Density estimate:\n",
    "$$\\tilde{p}(\\boldsymbol{x})=\\frac{k}{n V}=\\frac{k}{n \\cdot c_{d} \\cdot r_{k}^{d}(\\boldsymbol{x})}$$\n",
    "\n",
    "where $r_{k}(x)$ is the distance of x to its k-th nearest neighbour and $c_d \\cdot r_k(x)$ is the volume of a d-dimensional cube or sphere\n",
    "\n",
    "<br>\n",
    "\n",
    "#### kNN Estimate of Bayes Probability\n",
    "\n",
    "$$P\\left(\\omega_{i} | \\boldsymbol{x}\\right)=\\frac{p\\left(\\boldsymbol{x} | \\omega_{i}\\right) \\cdot p\\left(\\omega_{i}\\right)}{p(\\boldsymbol{x})}=\\frac{\\frac{k_{i}}{n_{i} V} \\cdot \\frac{n_{i}}{n}}{\\frac{k}{n V}}=\\frac{k_{i}}{k}$$\n",
    "\n",
    "Steps:\n",
    "- out of N training samples: identify the k nearest neighbours of x irrespective of the class label\n",
    "- identify the $k_i$ samples that belong to class $i$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<br><br>\n",
    "\n",
    "## Lecture 23: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Let $x^{(i)}$ be a feature vector compensated with its training mean $\\vec c$: $x^{(i)} \\leftarrow x^{(i)} - \\vec c$.\n",
    "\n",
    "We then try to maximize the projection of the dataset $x^{(i)}$ into a unit vector $U$ (which enforces a constaint $|U|=1$)\n",
    "\n",
    "\n",
    "$$ \\begin{align*} J=\\sum_{i}\\left(U^{\\top} x^{(i)}\\right)^{2}+\\lambda\\left(U^{\\top} U\\right) \\end{align*} $$\n",
    "With respect to $\\vec{M}$\n",
    "\n",
    "$$ \\begin{align*} \\begin{aligned} J &=\\sum_{i} U^{\\top}\\left(x^{(i)} x^{(i) \\top}\\right) U+\\lambda\\left(1-U^{\\top} v\\right) \\\\ & =U^{\\top} \\sum_{i}\\left(x^{(i)} x^{(i) \\, \\top} \\right) U+\\lambda\\left(1-U^{\\top} U\\right) \\\\ &= U^{\\top} S U+\\lambda\\left(1-U^{\\top} v\\right) \\end{aligned} \\end{align*} $$\n",
    "\n",
    "where $S$ is the $m\\times m$ symmetric covariance matrix $S=\\sum_{i} x^{(i)} x^{(i) \\top}$\n",
    "\n",
    "Then \n",
    "$$\\frac{\\partial J}{\\partial u}=2 S \\vec{U}-2 \\lambda \\vec{U} = 0, \\text{ and hence } S \\vec{U}=\\lambda \\vec{U}$$\n",
    "\n",
    "In other words $\\vec{U}$ is an eigenvector of $S$. Thus the variation\n",
    "\n",
    "$$\\begin{align*} \\sum_{i}\\left(U^{\\top} x^{(i)}\\right)^{2} = \\vec U^{\\top} S \\vec{S} = \\lambda \\underbrace{U^\\top U}_{I} \\end{align*} = \\lambda $$\n",
    "\n",
    "\n",
    "which implies that the cost function is maximized when $U$ is chosen as the eigenvector corresponding to the largest eigenvalue of $S$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<br>\n",
    "\n",
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Project data into another base where separability between classes is optimal\n",
    "\n",
    "The goal is to maximize the distance between classes after projection:\n",
    "\n",
    "$$ J(\\boldsymbol{w})=\\frac{\\left(\\tilde{m}_{1}-\\tilde{m}_{2}\\right)^{2}}{\\tilde{s}_{1}^{2}+\\tilde{s}_{2}^{2}} = \\frac{w^{\\top} S_{B} w}{w^{\\top} S_{W} w}$$\n",
    "\n",
    "where $m_i = \\mathbb E[x/w_i]$, $y = w^\\top x$ and $\\tilde{s}_{i}^{2}=\\sum_{y \\in \\tilde{\\omega}_{i}}\\left(y-\\tilde{m}_{i}\\right)^{2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- within class scatter matrix for class $i$ :\n",
    "$$ S_{i}=\\sum_{\\boldsymbol{x} \\in \\omega_{i}}\\left(\\boldsymbol{x}-\\boldsymbol{m}_{i}\\right)\\left(\\boldsymbol{x} -\\boldsymbol{m}_{i}\\right)^{\\top} $$\n",
    "\n",
    "- total within class scatter matrix:\n",
    "$$ S_{W}=S_{1}+S_{2} $$\n",
    "\n",
    "- between class scatter matrix:\n",
    "$$ S_{B}=\\left(m_{1}-m_{2}\\right)\\left(m_{1}-m_{2}\\right)^{\\top} $$\n",
    "\n",
    "\n",
    "#### Solution\n",
    "\n",
    "resulting formula:\n",
    "$$\\begin{align*} J(\\boldsymbol{w})=\\frac{\\boldsymbol{w}^{\\top} S_{B} \\boldsymbol{w}}{\\boldsymbol{w}^{\\top} S_{W} \\boldsymbol{w}} \\end{align*} $$\n",
    "\n",
    "- Maximise between class scatter\n",
    "- Minimise within class scatter\n",
    "\n",
    "leads to a generalized eigenvalue problem whose solution is\n",
    "$$ \\begin{align*} \\boldsymbol{w}=S_{W}^{-1}\\left(\\boldsymbol{m}_{1}-\\boldsymbol{m}_{2}\\right) \\end{align*}$$\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- if the classes are overlapping or their means is similar, LDA will be of little use\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}