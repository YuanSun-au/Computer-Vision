{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Recap - Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 20: Bayes Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "$$ P(w_j|x) = \\frac{ P(x|w_j) \\; P(w_j) }{ P(x) } $$ \n",
    "\n",
    "where the evidence $P(x)$ is a mixture of Gaussians (this is not a normal distribution anymore):\n",
    "$$ P(x) = \\sum_{k=1} P(x | w_k) P(w_k) $$\n",
    "\n",
    "\n",
    "PDF:\n",
    "$$f\\left(x_{1}, \\ldots, x_{m}\\right)=\\frac{1}{(2 \\pi)^{m / 2} \\sqrt{\\operatorname{det} \\Sigma}} e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\top} \\mathbf{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})}$$\n",
    "\n",
    "Mahalanobis Distance:\n",
    "$$ r = \\sqrt{ (x-\\mu)^\\top \\Sigma^{-1}(x-\\mu) } $$\n",
    "\n",
    "Curse of dimensionality:\n",
    "\n",
    "- if features are not independent, $\\color{red}{exponentially}$ more training data is needed to compute meaninful likelihoods\n",
    "- If features are independent: $P(x|w_j)=P(x_1,\\dots,x_d|w_j)=P(x_1|w_j)\\dots P(x_d|w_j)$, then the effort grows linearly with the dimension.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Bayesian Risk\n",
    "\n",
    "The cost (risk) of a decision is defined by a cost (loss) function $\\lambda(a,w_j|x)$ given by:\n",
    "\n",
    "$$ R[a|x] = \\mathbb E_{w_j \\sim p(w_j|x)}[\\lambda(a,w_j|x)] = \\sum_{j=1} \\lambda(a,w_j|x) \\, p(w_j|x) $$\n",
    "\n",
    "Examples of conditional risks:\n",
    "$$ R\\left(\\alpha_{1} | \\boldsymbol{x}\\right)=\\lambda_{11} P\\left(\\omega_{1} | \\boldsymbol{x}\\right)+\\lambda_{12} P\\left(\\omega_{2} | \\boldsymbol{x}\\right)$$\n",
    "$$ R\\left(\\alpha_{2} | \\boldsymbol{x}\\right)=\\lambda_{21} P\\left(\\omega_{1} | \\boldsymbol{x}\\right)+\\lambda_{22} P\\left(\\omega_{2} | \\boldsymbol{x}\\right)$$\n",
    "\n",
    "$\\lambda_{11}$ and $\\lambda_{22}$ are the cost for correct classification, that might not be zero, but at least we expect $\\lambda_{11} < \\lambda_{21}$ and $\\lambda_{22} < \\lambda_{12}$\n",
    "\n",
    "Bayes decision rule: decide on $w_j$ if $R(a_j|x) < R(a_k|x), \\;\\; \\forall k$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Likelihood Ratio Test (LRT)\n",
    "\n",
    "$$ \\Lambda(\\boldsymbol{x})=\\frac{P\\left(\\boldsymbol{x} | \\omega_{1}\\right)}{P\\left(\\boldsymbol{x} | \\omega_{2}\\right)} \\begin{array}{l} \\omega_{1} \\\\ >\\\\<\\\\ \\omega_{2} \\end{array} \\underbrace{\\frac{\\lambda_{12}-\\lambda_{22}}{\\lambda_{21}-\\lambda_{11}} \\cdot \\frac{P\\left(\\omega_{2}\\right)}{P\\left(\\omega_{1}\\right)}}_{T} $$\n",
    "\n",
    "if $\\lambda_{11}=\\lambda_{22}=1$ and $\\lambda_{12}=\\lambda_{21}=0$, then the LRT is called MAP criterion and ML for equal priors (T=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminant Based Classification\n",
    "\n",
    "Discriminant function: $g(x)$. Choosen class $i$ if \n",
    "$$ g_i(x) > g_j(x) \\quad \\forall i \\neq j $$\n",
    "\n",
    "Bayes rule can be used as a DF: $g_i(x) = P(w_i|x)$\n",
    "\n",
    "which implies that:\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\Sigma_{i}^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)-\\frac{d}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln \\left(\\left|\\Sigma_{i}\\right|\\right)+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 1: $\\Sigma_i=\\sigma I$\n",
    "\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{\\left\\|\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right\\|^{2}}{2 \\sigma}=-\\frac{x^{\\top} x-2 \\mu_i^{\\top} x+\\mu_i^{\\top} \\mu_i}{2 \\sigma}$$\n",
    "\n",
    "results in $\\color{red}{linear}$ discriminant:\n",
    "$$g_{i}(\\boldsymbol{x})=\\boldsymbol{w}_{i}^{\\top} \\boldsymbol{x}+\\boldsymbol{w}_{i 0} \\quad \\text{where}$$\n",
    "$$\\boldsymbol{w}_{i}=\\frac{1}{\\sigma} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2 \\sigma} \\boldsymbol{\\mu}_{i}^{\\top} \\boldsymbol{\\mu}_{i}+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 2: $\\Sigma_i=\\Sigma$\n",
    "\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\Sigma_{i}^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)$$\n",
    "\n",
    "which is also a $\\color{red}{linear}$ discriminant:\n",
    "$$g_{i}(\\boldsymbol{x})=\\boldsymbol{w}_{i}^{\\top} \\boldsymbol{x}+\\boldsymbol{w}_{i 0} \\quad \\text{where}$$\n",
    "$$\\boldsymbol{w}_{i}=\\Sigma^{-1} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2} \\boldsymbol{\\mu}_{i}^{\\top} \\Sigma^{-1} \\boldsymbol{\\mu}_{i}+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 3: $\\Sigma_i$ = Arbitrary\n",
    "\n",
    "$$ g_i(x) = x^\\top W_i x + w_i^\\top x + w_{i,0} \\quad \\text{where}$$\n",
    "$$W_{i}=-\\frac{1}{2} \\Sigma_{i}^{-1}, \\quad \\boldsymbol{w}_{i}=\\Sigma_{i}^{-1} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2} \\boldsymbol{\\mu}_{i}^{\\top} \\Sigma_{i}^{-1} \\boldsymbol{\\mu}_{i}-\\frac{1}{2} \\ln \\left|\\Sigma_{i}\\right|+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "which is $\\color{red}{quadratic}$ and non-contiguous\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Lecture 21: Parametric Techniques, Density Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Estimation\n",
    "\n",
    "#### Maximum Likelihood\n",
    "\n",
    "If samples are drawn independently\n",
    "$$ l(\\boldsymbol{\\theta})=\\ln p(D | \\boldsymbol{\\theta})=\\ln \\prod_{k=1}^{n} p\\left(\\boldsymbol{x}_{k} | \\boldsymbol{\\theta}\\right)=\\sum_{k=1}^{n} \\ln p\\left(\\boldsymbol{x}_{k} | \\boldsymbol{\\theta}\\right) $$\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}}=\\arg \\max _{\\boldsymbol{\\theta}} l(\\boldsymbol{\\theta})$$\n",
    "\n",
    "Maximizing the likelihood implies that:\n",
    "$$\\nabla_{\\theta} l(\\theta)=\\nabla_{\\theta} \\sum_{k=1}^{n} \\ln p\\left(x_{k} | \\theta\\right)=\\sum_{k=1}^{n}\\nabla_{\\theta} \\ln p\\left(x_{k} | \\theta\\right)=0$$\n",
    "\n",
    "If Gaussian case and unknown mean $\\mu$ and unknown covariance $\\Sigma$, then:\n",
    "\n",
    "$$\\hat{\\mu}=\\frac{1}{n} \\sum_{k=1}^{m} x_{k} \\quad \\text{and} \\quad \\hat{\\Sigma}=\\frac{1}{n} \\sum_{k=1}^{n}\\left(x_{k}-\\mu\\right)\\left(x_{k}-\\mu\\right)^{\\top}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Density Estimation\n",
    "\n",
    "Idea: Estimate the distribution (function) from scratch\n",
    "\n",
    "$$P=\\int_{R} p(x) d x \\approx p\\left(x^{*}\\right) \\operatorname{vol}(R) \\approx \\frac{k}{n}$$\n",
    "\n",
    "where $k$ out of $n$ samples fall into the range $R$\n",
    "\n",
    "Choose a large enough R, so that it contains sufficiently many samples and (so that variance/uncertainty is not so large)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/densest.png\" width=400>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "To estimate distribution at $x^*$:\n",
    "$$p_{n}\\left(x^{*}\\right)=\\frac{k_{n} / n}{\\operatorname{vol}\\left(R_{n}\\right)} \\approx p\\left(x^{*}\\right)$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Bias-Variance Tradeoff\n",
    "\n",
    "<img src=\"./images/biasvariance.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Lecture 22: Non-Parametric Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parzen Windows\n",
    "\n",
    "**Idea:** Count the number of samples $k$ within a region $R$ of fixed size\n",
    "\n",
    "Define a funcion:\n",
    "$$\\varphi(u)=\\left\\{ \\begin{array}{ll} 1, & \\left|u_{j}\\right| \\leq \\frac{1}{2}, j=1,2, \\ldots, d \\\\ 0, & \\text { otherwise }\\end{array} \\right.$$\n",
    "\n",
    "The previous definitions for the density and the number of samples\n",
    "$$\\begin{align*} p(\\boldsymbol{x}) \\approx \\frac{k_{n} / n}{\\operatorname{vol}\\left(R_{n}\\right)} \\quad \\text { and } \\quad k_{n}=\\sum_{i=1}^{n} \\varphi\\left(\\frac{\\boldsymbol{x}-\\boldsymbol{x}_{i}}{h}\\right) \\end{align*}$$\n",
    "\n",
    "in a region $R$ lead to the density estimate\n",
    "$$ \\begin{align*} \\tilde{p}(\\boldsymbol{x}) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{\\operatorname{vol}(R)} \\varphi\\left(\\frac{x-x_{i}}{h}\\right)=\\frac{1}{n h^{d}} \\sum_{i=1}^{n} \\varphi\\left(\\frac{x-x_{i}}{h}\\right) \\end{align*} $$\n",
    "\n",
    "### General Parzen Windows\n",
    "\n",
    "Change kernel function $\\varphi((x-x_i)/h)$ by a Gaussian centered at $x_i$ with variance $h$\n",
    "\n",
    "<img src=\"./images/parzen.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours (KNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Lecture 23: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}