{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Recap - Pattern Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 20: Bayes Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "$$ P(w_j|x) = \\frac{ P(x|w_j) \\; P(w_j) }{ P(x) } $$ \n",
    "\n",
    "where the evidence $P(x)$ is a mixture of Gaussians (this is not a normal distribution anymore):\n",
    "$$ P(x) = \\sum_{k=1} P(x | w_k) P(w_k) $$\n",
    "\n",
    "\n",
    "PDF:\n",
    "$$f\\left(x_{1}, \\ldots, x_{m}\\right)=\\frac{1}{(2 \\pi)^{m / 2} \\sqrt{\\operatorname{det} \\Sigma}} e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\top} \\mathbf{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})}$$\n",
    "\n",
    "Mahalanobis Distance:\n",
    "$$ r = \\sqrt{ (x-\\mu)^\\top \\Sigma^{-1}(x-\\mu) } $$\n",
    "\n",
    "Curse of dimensionality:\n",
    "\n",
    "- if features are not independent, $\\color{red}{exponentially}$ more training data is needed to compute meaninful likelihoods\n",
    "- If features are independent: $P(x|w_j)=P(x_1,\\dots,x_d|w_j)=P(x_1|w_j)\\dots P(x_d|w_j)$, then the effort grows linearly with the dimension.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Bayesian Risk\n",
    "\n",
    "The cost (risk) of a decision is defined by a cost (loss) function $\\lambda()$ given by:\n",
    "\n",
    "$$ R[a|x] = \\mathbb E_{w_j \\sim p(w_j|x)}[\\lambda(a,w_j|x)] = \\sum_{j=1} \\lambda(a,w_j|x) \\, p(w_j|x) $$\n",
    "\n",
    "Examples of conditional risks:\n",
    "$$ R\\left(\\alpha_{1} | \\boldsymbol{x}\\right)=\\lambda_{11} P\\left(\\omega_{1} | \\boldsymbol{x}\\right)+\\lambda_{12} P\\left(\\omega_{2} | \\boldsymbol{x}\\right)$$\n",
    "$$ R\\left(\\alpha_{2} | \\boldsymbol{x}\\right)=\\lambda_{21} P\\left(\\omega_{1} | \\boldsymbol{x}\\right)+\\lambda_{22} P\\left(\\omega_{2} | \\boldsymbol{x}\\right)$$\n",
    "\n",
    "$\\lambda_{11}$ and $\\lambda_{22}$ are the cost for correct classification, that might not be zero, but at least we expect $\\lambda_{11} < \\lambda_{21}$ and $\\lambda_{22} < \\lambda_{12}$\n",
    "\n",
    "Bayes decision rule: decide on $w_j$ if $R(a_j|x) < R(a_k|x), \\;\\; \\forall k$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Likelihood Ratio Test (LRT)\n",
    "\n",
    "$$ \\Lambda(\\boldsymbol{x})=\\frac{P\\left(\\boldsymbol{x} | \\omega_{1}\\right)}{P\\left(\\boldsymbol{x} | \\omega_{2}\\right)} \\begin{array}{l} \\omega_{1} \\\\ >\\\\<\\\\ \\omega_{2} \\end{array} \\underbrace{\\frac{\\lambda_{12}-\\lambda_{22}}{\\lambda_{21}-\\lambda_{11}} \\cdot \\frac{P\\left(\\omega_{2}\\right)}{P\\left(\\omega_{1}\\right)}}_{T} $$\n",
    "\n",
    "if $\\lambda_{11}=\\lambda_{22}=1$ and $\\lambda_{12}=\\lambda_{21}=0$, then the LRT is called MAP criterion and ML for equal priors (T=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminant Based Classification\n",
    "\n",
    "Discriminant function: $g(x)$. Choosen class $i$ if \n",
    "$$ g_i(x) > g_j(x) \\quad \\forall i \\neq j $$\n",
    "\n",
    "Bayes rule can be used as a DF: $g_i(x) = P(w_i|x)$\n",
    "\n",
    "which implies that:\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\Sigma_{i}^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)-\\frac{d}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln \\left(\\left|\\Sigma_{i}\\right|\\right)+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 1: $\\Sigma_i=\\sigma I$\n",
    "\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{\\left\\|\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right\\|^{2}}{2 \\sigma}=-\\frac{x^{\\top} x-2 \\mu_i^{\\top} x+\\mu_i^{\\top} \\mu_i}{2 \\sigma}$$\n",
    "\n",
    "results in $\\color{red}{linear}$ discriminant:\n",
    "$$g_{i}(\\boldsymbol{x})=\\boldsymbol{w}_{i}^{\\top} \\boldsymbol{x}+\\boldsymbol{w}_{i 0} \\quad \\text{where}$$\n",
    "$$\\boldsymbol{w}_{i}=\\frac{1}{\\sigma} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2 \\sigma} \\boldsymbol{\\mu}_{i}^{\\top} \\boldsymbol{\\mu}_{i}+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 2: $\\Sigma_i=\\Sigma$\n",
    "\n",
    "$$g_{i}(\\boldsymbol{x})=-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\Sigma_{i}^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)$$\n",
    "\n",
    "which is also a $\\color{red}{linear}$ discriminant:\n",
    "$$g_{i}(\\boldsymbol{x})=\\boldsymbol{w}_{i}^{\\top} \\boldsymbol{x}+\\boldsymbol{w}_{i 0} \\quad \\text{where}$$\n",
    "$$\\boldsymbol{w}_{i}=\\Sigma^{-1} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2} \\boldsymbol{\\mu}_{i}^{\\top} \\Sigma^{-1} \\boldsymbol{\\mu}_{i}+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "#### Case 3: $\\Sigma_i$ = Arbitrary\n",
    "\n",
    "$$ g_i(x) = x^\\top W_i x + w_i^\\top x + w_{i,0} \\quad \\text{where}$$\n",
    "$$W_{i}=-\\frac{1}{2} \\Sigma_{i}^{-1}, \\quad \\boldsymbol{w}_{i}=\\Sigma_{i}^{-1} \\boldsymbol{\\mu}_{i}, \\quad w_{i 0}=-\\frac{1}{2} \\boldsymbol{\\mu}_{i}^{\\top} \\Sigma_{i}^{-1} \\boldsymbol{\\mu}_{i}-\\frac{1}{2} \\ln \\left|\\Sigma_{i}\\right|+\\ln P\\left(\\omega_{i}\\right)$$\n",
    "\n",
    "which is $\\color{red}{quadratic}$ and non-contiguous\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}